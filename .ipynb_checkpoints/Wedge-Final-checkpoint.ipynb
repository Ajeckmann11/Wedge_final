{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wedge Data Engineering Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import random\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = os.listdir(\"clean-files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These first two values will be different on your machine.\n",
    "service_path = \"/Users/ajeck/Documents/ADA/Wedge Project/\"\n",
    "service_file = 'WedgeProject-843a50287012.json'\n",
    "gbq_proj_id = 'wedgeproject-290522'\n",
    "gbq_dataset_id = 'wedge'\n",
    "\n",
    "private_key =service_path + service_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a client with my credentials so I can talk to my GBQ\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beginning code that allows us to modify the table\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "job_config.schema_update_options = [bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema field additions for tables\n",
    "job_config.schema = [\n",
    "    bigquery.SchemaField(\"datetime\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"register_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"emp_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"upc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"description\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_subtype\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_status\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"department\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"quantity\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"Scale\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cost\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"unitPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"total\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"regPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"altPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tax\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"taxexempt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"foodstamp\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"wicable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discountable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discounttype\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"voided\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"percentDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"ItemQtty\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volDiscType\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volume\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"VolSpecial\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"mixMatch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"matched\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memType\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"staff\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"numflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"itemstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tenderstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"charflag\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"varflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"batchHeaderID\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"local\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"organic\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"display\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"receipt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"card_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"store\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"branch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"match_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "]\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "job_config.skip_leading_rows = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texists(client, table_ref):\n",
    "    from google.cloud.exceptions import NotFound\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transArchive_201001_201003_clean.csv', 'transArchive_201004_201006_clean.csv', 'transArchive_201007_201009_clean.csv', 'transArchive_201010_201012_clean.csv', 'transArchive_201101_201103_clean.csv', 'transArchive_201104_clean.csv', 'transArchive_201105_clean.csv', 'transArchive_201106_clean.csv', 'transArchive_201107_201109_clean.csv', 'transArchive_201110_201112_clean.csv', 'transArchive_201201_201203_clean.csv', 'transArchive_201201_201203_inactive_clean.csv', 'transArchive_201204_201206_clean.csv', 'transArchive_201204_201206_inactive_clean.csv', 'transArchive_201207_201209_clean.csv', 'transArchive_201207_201209_inactive_clean.csv', 'transArchive_201210_201212_clean.csv', 'transArchive_201210_201212_inactive_clean.csv', 'transArchive_201301_201303_clean.csv', 'transArchive_201301_201303_inactive_clean.csv', 'transArchive_201304_201306_clean.csv', 'transArchive_201304_201306_inactive_clean.csv', 'transArchive_201307_201309_clean.csv', 'transArchive_201307_201309_inactive_clean.csv']\n"
     ]
    }
   ],
   "source": [
    "#Had to use to rewrite first files that got written in twice\n",
    "#fixclean = cleaned[:24]\n",
    "#print(fixclean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table transArchive_201001_201003 contains 0 columns\n",
      "Finished loading 2998329 rows into wedge_example:transArchive_201001_201003.\n",
      "Finished loading transArchive_201001_201003 now contains 50 columns.\n",
      "Table transArchive_201004_201006 contains 0 columns\n",
      "Finished loading 3185806 rows into wedge_example:transArchive_201004_201006.\n",
      "Finished loading transArchive_201004_201006 now contains 50 columns.\n",
      "Table transArchive_201007_201009 contains 0 columns\n",
      "Finished loading 2992584 rows into wedge_example:transArchive_201007_201009.\n",
      "Finished loading transArchive_201007_201009 now contains 50 columns.\n",
      "Table transArchive_201010_201012 contains 0 columns\n",
      "Finished loading 2957585 rows into wedge_example:transArchive_201010_201012.\n",
      "Finished loading transArchive_201010_201012 now contains 50 columns.\n",
      "Table transArchive_201101_201103 contains 0 columns\n",
      "Finished loading 2920825 rows into wedge_example:transArchive_201101_201103.\n",
      "Finished loading transArchive_201101_201103 now contains 50 columns.\n",
      "Table transArchive_201104 contains 0 columns\n",
      "Finished loading 1066333 rows into wedge_example:transArchive_201104.\n",
      "Finished loading transArchive_201104 now contains 50 columns.\n",
      "Table transArchive_201105 contains 0 columns\n",
      "Finished loading 1068514 rows into wedge_example:transArchive_201105.\n",
      "Finished loading transArchive_201105 now contains 50 columns.\n",
      "Table transArchive_201106 contains 0 columns\n",
      "Finished loading 992905 rows into wedge_example:transArchive_201106.\n",
      "Finished loading transArchive_201106 now contains 50 columns.\n",
      "Table transArchive_201107_201109 contains 0 columns\n",
      "Finished loading 3011934 rows into wedge_example:transArchive_201107_201109.\n",
      "Finished loading transArchive_201107_201109 now contains 50 columns.\n",
      "Table transArchive_201110_201112 contains 0 columns\n",
      "Finished loading 3121116 rows into wedge_example:transArchive_201110_201112.\n",
      "Finished loading transArchive_201110_201112 now contains 50 columns.\n",
      "Table transArchive_201201_201203 contains 0 columns\n",
      "Finished loading 2989643 rows into wedge_example:transArchive_201201_201203.\n",
      "Finished loading transArchive_201201_201203 now contains 50 columns.\n",
      "Table transArchive_201201_201203_inactive contains 0 columns\n",
      "Finished loading 245771 rows into wedge_example:transArchive_201201_201203_inactive.\n",
      "Finished loading transArchive_201201_201203_inactive now contains 50 columns.\n",
      "Table transArchive_201204_201206 contains 0 columns\n",
      "Finished loading 3083545 rows into wedge_example:transArchive_201204_201206.\n",
      "Finished loading transArchive_201204_201206 now contains 50 columns.\n",
      "Table transArchive_201204_201206_inactive contains 0 columns\n",
      "Finished loading 237989 rows into wedge_example:transArchive_201204_201206_inactive.\n",
      "Finished loading transArchive_201204_201206_inactive now contains 50 columns.\n",
      "Table transArchive_201207_201209 contains 0 columns\n",
      "Finished loading 2925607 rows into wedge_example:transArchive_201207_201209.\n",
      "Finished loading transArchive_201207_201209 now contains 50 columns.\n",
      "Table transArchive_201207_201209_inactive contains 0 columns\n",
      "Finished loading 190876 rows into wedge_example:transArchive_201207_201209_inactive.\n",
      "Finished loading transArchive_201207_201209_inactive now contains 50 columns.\n",
      "Table transArchive_201210_201212 contains 0 columns\n",
      "Finished loading 2893636 rows into wedge_example:transArchive_201210_201212.\n",
      "Finished loading transArchive_201210_201212 now contains 50 columns.\n",
      "Table transArchive_201210_201212_inactive contains 0 columns\n",
      "Finished loading 162987 rows into wedge_example:transArchive_201210_201212_inactive.\n",
      "Finished loading transArchive_201210_201212_inactive now contains 50 columns.\n",
      "Table transArchive_201301_201303 contains 0 columns\n",
      "Finished loading 2903986 rows into wedge_example:transArchive_201301_201303.\n",
      "Finished loading transArchive_201301_201303 now contains 50 columns.\n",
      "Table transArchive_201301_201303_inactive contains 0 columns\n",
      "Finished loading 148622 rows into wedge_example:transArchive_201301_201303_inactive.\n",
      "Finished loading transArchive_201301_201303_inactive now contains 50 columns.\n",
      "Table transArchive_201304_201306 contains 0 columns\n",
      "Finished loading 3025433 rows into wedge_example:transArchive_201304_201306.\n",
      "Finished loading transArchive_201304_201306 now contains 50 columns.\n",
      "Table transArchive_201304_201306_inactive contains 0 columns\n",
      "Finished loading 137627 rows into wedge_example:transArchive_201304_201306_inactive.\n",
      "Finished loading transArchive_201304_201306_inactive now contains 50 columns.\n",
      "Table transArchive_201307_201309 contains 0 columns\n",
      "Finished loading 2997134 rows into wedge_example:transArchive_201307_201309.\n",
      "Finished loading transArchive_201307_201309 now contains 50 columns.\n",
      "Table transArchive_201307_201309_inactive contains 0 columns\n",
      "Finished loading 104467 rows into wedge_example:transArchive_201307_201309_inactive.\n",
      "Finished loading transArchive_201307_201309_inactive now contains 50 columns.\n"
     ]
    }
   ],
   "source": [
    "#open the files and loading them into seperate tables in GBQ\n",
    "filepath = \"/Users/ajeck/Documents/ADA/Wedge Project/clean-files/\"\n",
    "for file in cleaned:\n",
    "    #just to grab everything pre-_clean (which is the table part)\n",
    "    tab, other = file.split(\"_clean\")\n",
    "    table_full_name = \".\".join([gbq_proj_id,gbq_dataset_id,tab])\n",
    "    \n",
    "    if not texists(client, table_full_name) :\n",
    "        table_ref = client.create_table(table = table_full_name)\n",
    "    else :\n",
    "        table_ref = client.get_table(table_full_name)\n",
    "    \n",
    "    table = client.get_table(table_ref)\n",
    "    print(\"Table {} contains {} columns\".format(table_ref.table_id,len(table.schema)))\n",
    "    \n",
    "    with open(filepath + file, \"rb\") as my_file:\n",
    "        job = client.load_table_from_file(\n",
    "            my_file,\n",
    "            table_ref,\n",
    "            location = \"US\",\n",
    "            job_config=job_config,\n",
    "        )\n",
    "\n",
    "    job.result()\n",
    "    \n",
    "    print(\"Finished loading {} rows into {}:{}.\".format(job.output_rows, 'wedge_example', table_ref.table_id))\n",
    "    table = client.get_table(table)\n",
    "    print(\"Finished loading {} now contains {} columns.\".format(table_ref.table_id, len(table.schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the distinct individual card_no except the coding for non-members, card_no=3\n",
    "query_card_nos = \"Select Distinct card_no,\" \"From `wedgeproject-290522.wedge.transArchive*`\" \"Where card_no !=3\"\n",
    "query_get_card_nos = client.query(query_card_nos,location ='US',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "owners = []\n",
    "\n",
    "for idx, row in enumerate(query_get_card_nos) :\n",
    "    card_no = row[0]\n",
    "    \n",
    "    owners.append(card_no)\n",
    "\n",
    "print(len(owners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "random.seed(312)\n",
    "samp_size = 250\n",
    "samp_owners = random.choices(owners, k=samp_size)\n",
    "print(len(samp_owners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query selecting card numbers\n",
    "owners_join = \",\".join([str(owner) for owner in samp_owners])\n",
    "query = \"\"\"Select * From `wedgeproject-290522.wedge.transArchive*` Where card_no in (\"\"\" + owners_join + \")\"\n",
    "query_own = client.query(query,location ='US',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the files as txt file and add headers to the first row\n",
    "headers = [\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"]\n",
    "with open (\"sample_of_owners.txt\", \"w\") as outfile :\n",
    "    outfile.write (\"\\t\".join(headers) + \"\\n\")\n",
    "    for row in query_own:\n",
    "        outfile.write(\"\\t\".join([str(item) for item in row]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "## Sales by Date by Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want by distinct hour (calendar day and hour combo), get sum of sales (total spend), count transactions and sum items\n",
    "# do this by extracting date and hour from datetime, and taking sum of sales\n",
    "# then we have to count the distict occurances of date, transaction, employee and register as unique transactions, then we can use count\n",
    "# lastly, we sum the items sold, but -1 for the returns and voids because these items need to be counted against an item sold\n",
    "# for example, if an item is double scanned, then voided we need -1 so that the two items from the double scan only counts as 1\n",
    "\n",
    "query = (\"SELECT (EXTRACT(date FROM datetime)) AS Date, \" \n",
    "\"(EXTRACT(hour FROM datetime)) AS Hour, \" \n",
    "\"SUM(total) AS Sales, \" \n",
    "\"COUNT(DISTINCT(Date(datetime) || trans_no || emp_no || register_no)) AS Transactions, \" \n",
    "\"SUM(CASE WHEN(trans_status = 'R' OR trans_status = 'V') THEN -1 ELSE 1 END) as Items \"  \n",
    "\"FROM `wedgeproject-290522.wedge.transArchive*` \" \n",
    "\"WHERE card_no != 3 \" \n",
    "\"AND department != 0 \" \n",
    "\"AND department != 15 \" \n",
    "\"AND trans_status != 'M' \" \n",
    "\"AND trans_status != 'C' \" \n",
    "\"AND trans_status != 'J' \" \n",
    "\"AND (trans_status = '' \" \n",
    "\"OR trans_status = ' ' \" \n",
    "\"OR trans_status =  'V' \" \n",
    "\"OR trans_status = 'R') \" \n",
    "\"GROUP BY Date, Hour \" \n",
    "\"ORDER BY Date, Hour \")\n",
    "\n",
    "#execute query\n",
    "query_sales_by_date_by_hour = client.query(query,location ='US',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"date\", \"hours\", \"sales\", \"transactions\", \"items\"]\n",
    "with open (\"sales_by_date_by_hour.txt\", \"w\") as outfile :\n",
    "    outfile.write (\"\\t\".join(headers) + \"\\n\")\n",
    "    for row in query_sales_by_date_by_hour:\n",
    "        newlist = [str(item) for item in row]\n",
    "        outfile.write(\"\\t\".join(newlist) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "db = sqlite3.connect(\"wedge_aj.db\")\n",
    "cur = db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create table in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"sales_by_date_by_hour.txt\"\n",
    "cur.execute('''DROP TABLE IF EXISTS sales_by_date_by_hour''')\n",
    "cur.execute('''CREATE TABLE sales_by_date_by_hour (\n",
    "    date TIMESTAMP, \n",
    "    hour INTEGER, \n",
    "    sales INTEGER,\n",
    "    transactions INTEGER,\n",
    "    items INTEGER)''')\n",
    "\n",
    "with open(input_txt,'r', encoding=\"Latin-1\") as inputfile :\n",
    "    next(inputfile)\n",
    "    for idx, line in enumerate(inputfile.readlines()) :\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        cur.execute('''\n",
    "            INSERT INTO sales_by_date_by_hour (date,hour,sales,transactions,items)\n",
    "            VALUES (?,?,?,?,?)''', line)\n",
    "\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales by Owner by Year by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want by distinct month (calendar year and month combo), get sum of sales (total), count transactions and sum items\n",
    "# do this by extracting year and month from datetime, and taking sum of sales\n",
    "# then we have to count the distict occurances of date, transaction, employee and register as unique transactions, then we can use count\n",
    "# lastly, we sum the items sold, but -1 for the returns and voids because these items need to be counted against an item sold\n",
    "# for example, if an item is double scanned, then voided we need -1 so that the two items from the double scan only counts as 1\n",
    "\n",
    "query = ('''SELECT card_no As Owner,\n",
    "    (EXTRACT(year FROM datetime)) AS Year,\n",
    "    (EXTRACT(month FROM datetime)) AS Month,\n",
    "    SUM(total) AS Sales,\n",
    "    COUNT(DISTINCT(Date(datetime) || trans_no || emp_no || register_no)) AS Transactions,\n",
    "    SUM(CASE WHEN(trans_status = 'R' OR trans_status = 'V') THEN -1 ELSE 1 END) as Items\n",
    "    FROM `wedgeproject-290522.wedge.transArchive*`\n",
    "    WHERE card_no != 3\n",
    "    AND department != 0\n",
    "    AND department != 15\n",
    "    AND trans_status != 'M'\n",
    "    AND trans_status != 'C'\n",
    "    AND trans_status != 'J'\n",
    "    AND (trans_status = ''\n",
    "    OR trans_status = ' '\n",
    "    OR trans_status =  'V'\n",
    "    OR trans_status = 'R')\n",
    "    GROUP BY Owner, Year, Month\n",
    "    ORDER BY Owner, Year, Month DESC''')\n",
    "\n",
    "#execute query\n",
    "query_sales_by_owner_by_year_by_month = client.query(query,location ='US',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"owner\",\"year\",\"month\",\"sales\",\"transactions\",\"items\"]\n",
    "with open (\"sales_by_owner_by_year_by_month.txt\", \"w\") as outfile :\n",
    "    outfile.write (\"\\t\".join(headers) + \"\\n\")\n",
    "    for row in query_sales_by_owner_by_year_by_month:\n",
    "        newlist = [str(item) for item in row]\n",
    "        outfile.write(\"\\t\".join(newlist) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create table in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"sales_by_owner_by_year_by_month.txt\"\n",
    "cur.execute('''DROP TABLE IF EXISTS sales_by_owner_by_year_by_month''')\n",
    "cur.execute('''CREATE TABLE sales_by_owner_by_year_by_month (\n",
    "    owner INTEGER, year INTEGER, month INTEGER, sales INTEGER, transactions INTEGER, items INTEGER)''')\n",
    "\n",
    "with open(input_txt,'r', encoding=\"Latin-1\") as inputfile :\n",
    "    next(inputfile)\n",
    "    for idx, line in enumerate(inputfile.readlines()) :\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        cur.execute('''\n",
    "            INSERT INTO sales_by_owner_by_year_by_month (owner,year,month,sales,transactions,items)\n",
    "            VALUES (?,?,?,?,?,?)''', line)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales by Product Description by Year by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are selecting, department, product desription, extracting year and month from datetime and grabbing the universal product key\n",
    "# The desired calculated values are sales, derived by sum of total, then distinct transactions and items just as before\n",
    "query = ('''SELECT deplook.dept_name, trans.department, description,\n",
    "    (EXTRACT(year FROM datetime)) AS Year,\n",
    "    (EXTRACT(month FROM datetime)) AS Month,\n",
    "    upc,\n",
    "    SUM(total) AS Sales,\n",
    "    COUNT(DISTINCT(Date(datetime) || trans_no || emp_no || register_no)) AS Transactions,\n",
    "    SUM(CASE WHEN(trans_status = 'R' OR trans_status = 'V') THEN -1 ELSE 1 END) as Items,\n",
    "    trans.department AS dept_no\n",
    "    FROM `wedgeproject-290522.wedge.transArchive*` AS trans\n",
    "    LEFT OUTER JOIN `wedgeproject-290522.wedge.department_lookup` AS deplook ON trans.department = deplook.department\n",
    "    WHERE card_no != 3\n",
    "    AND trans.department != 0 \n",
    "    AND trans.department != 15\n",
    "    AND trans_status != 'M'\n",
    "    AND trans_status != 'C'\n",
    "    AND trans_status != 'J'\n",
    "    AND (trans_status = ''\n",
    "    OR trans_status = ' '\n",
    "    OR trans_status =  'V'\n",
    "    OR trans_status = 'R')\n",
    "    GROUP BY Year, Month, upc, description, dept_no, deplook.dept_name\n",
    "    ORDER BY description, Year, Month DESC''')\n",
    "\n",
    "#execute query\n",
    "query_sales_by_product_description_by_year_by_month = client.query(query,location ='US',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"department\",\"dept_name\",\"description\", \"year\",\"month\", \"upc\", \"sales\", \"transactions\", \"items\",\"dept_no\"]\n",
    "with open (\"sales_by_product_description_by_year_by_month.txt\", \"w\") as outfile :\n",
    "    outfile.write (\"\\t\".join(headers) + \"\\n\")\n",
    "    for row in query_sales_by_product_description_by_year_by_month:\n",
    "        newlist = [str(item) for item in row]\n",
    "        outfile.write(\"\\t\".join(newlist) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"sales_by_product_description_by_year_by_month.txt\"\n",
    "cur.execute('''DROP TABLE IF EXISTS sales_by_product_description_by_year_by_month''')\n",
    "cur.execute('''CREATE TABLE sales_by_product_description_by_year_by_month (\n",
    "    Department INTEGER,\n",
    "    Dept_name INTERGER,\n",
    "    Description INTERGER,\n",
    "    Year INTEGER,\n",
    "    Month INTEGER,\n",
    "    upc INTEGER,\n",
    "    Sales INTEGER,\n",
    "    Transactions INTEGER,\n",
    "    Items INTEGER,\n",
    "    dept_no INTEGER)''')\n",
    "\n",
    "with open(input_txt,'r', encoding=\"Latin-1\") as inputfile :\n",
    "    next(inputfile)\n",
    "    for idx, line in enumerate(inputfile.readlines()) :\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        cur.execute('''\n",
    "            INSERT INTO sales_by_product_description_by_year_by_month (Department,Dept_name,Description,Year,Month,upc,Sales,Transactions,Items,dept_no)\n",
    "            VALUES (?,?,?,?,?,?,?,?,?,?)''', line)\n",
    "\n",
    "db.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
